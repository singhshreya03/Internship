{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853dc049",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     25\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     rank \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     27\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m     artist \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the table containing the most-viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Extract data from each row in the table\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[3].text.strip()\n",
    "    views = columns[4].text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print or further process the extracted data\n",
    "for rank, name, artist, upload_date, views in zip(rank_list, name_list, artist_list, upload_date_list, views_list):\n",
    "    print(\"Rank:\", rank)\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Artist:\", artist)\n",
    "    print(\"Upload Date:\", upload_date)\n",
    "    print(\"Views:\", views)\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0411651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/lw6f6b7d16q2_yf4k_8hbpch0000gn/T/ipykernel_19486/1377729434.py:14: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  fixtures_link = soup.find(\"a\", text=\"INTERNATIONAL FIXTURES\")[\"href\"]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the international fixtures page\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fixtures_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTERNATIONAL FIXTURES\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Construct the full URL of the international fixtures page\u001b[39;00m\n\u001b[1;32m     17\u001b[0m fixtures_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bcci.tv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfixtures_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI homepage\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the BCCI homepage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find(\"a\", text=\"INTERNATIONAL FIXTURES\")[\"href\"]\n",
    "\n",
    "# Construct the full URL of the international fixtures page\n",
    "fixtures_url = f\"https://www.bcci.tv{fixtures_link}\"\n",
    "\n",
    "# Send a GET request to the international fixtures page\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "# Parse the HTML content of the fixtures page\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.text, \"html.parser\")\n",
    "\n",
    "# Find the container with the fixtures\n",
    "fixtures_container = fixtures_soup.find(\"div\", class_=\"js-list\")\n",
    "\n",
    "# Extract details for each fixture\n",
    "for fixture in fixtures_container.find_all(\"div\", class_=\"event-list__list\"):\n",
    "    series = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "    place = fixture.find(\"p\", class_=\"fixture__additional-info\").next_sibling.text.strip()\n",
    "    date = fixture.find(\"span\", class_=\"fixture__datetime tablet-only\").text.strip()\n",
    "    time = fixture.find(\"span\", class_=\"fixture__datetime\").text.strip()\n",
    "\n",
    "    # Print the details\n",
    "    print(\"Series:\", series)\n",
    "    print(\"Place:\", place)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Time:\", time)\n",
    "    print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc62475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/lw6f6b7d16q2_yf4k_8hbpch0000gn/T/ipykernel_19486/4289038042.py:14: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  indian_economy_link = soup.find(\"a\", text=\"Economy of India\")[\"href\"]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the Indian Economy section\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m indian_economy_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEconomy of India\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Construct the full URL of the Indian Economy section\u001b[39;00m\n\u001b[1;32m     17\u001b[0m indian_economy_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mindian_economy_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the link to the Indian Economy section\n",
    "indian_economy_link = soup.find(\"a\", text=\"Economy of India\")[\"href\"]\n",
    "\n",
    "# Construct the full URL of the Indian Economy section\n",
    "indian_economy_url = f\"{url}{indian_economy_link}\"\n",
    "\n",
    "# Send a GET request to the Indian Economy section\n",
    "response = requests.get(indian_economy_url)\n",
    "\n",
    "# Parse the HTML content of the Indian Economy section\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the link to the Indian States section\n",
    "indian_states_link = soup.find(\"a\", text=\"GDP of Indian states\")[\"href\"]\n",
    "\n",
    "# Construct the full URL of the Indian States section\n",
    "indian_states_url = f\"{url}{indian_states_link}\"\n",
    "\n",
    "# Send a GET request to the Indian States section\n",
    "response = requests.get(indian_states_url)\n",
    "\n",
    "# Parse the HTML content of the Indian States section\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the table containing the state-wise GDP details\n",
    "table = soup.find(\"table\", class_=\"display\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_list = []\n",
    "gsdp_19_20_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Extract data from each row in the table\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_18_19 = columns[2].text.strip()\n",
    "    gsdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_18_19_list.append(gsdp_18_19)\n",
    "    gsdp_19_20_list.append(gsdp_19_20)\n",
    "    share_18_19_list.append(share_18_19)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print or further process the extracted data\n",
    "for rank, state, gsdp_18_19, gsdp_19_20, share_18_19, gdp_billion in zip(rank_list, state_list, gsdp_18_19_list, gsdp_19_20_list, share_18_19_list, gdp_billion_list):\n",
    "    print(\"Rank:\", rank)\n",
    "    print(\"State:\", state)\n",
    "    print(\"GSDP(18-19) - at current prices:\", gsdp_18_19)\n",
    "    print(\"GSDP(19-20) - at current prices:\", gsdp_19_20)\n",
    "    print(\"Share(18-19):\", share_18_19)\n",
    "    print(\"GDP($ billion):\", gdp_billion)\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae0abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find trending repositories container.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the container with trending repositories\n",
    "trending_repos_container = soup.find('ol', class_='repo-list')\n",
    "\n",
    "if trending_repos_container:\n",
    "    # Find all individual repositories in the container\n",
    "    repos = trending_repos_container.find_all('li', class_='col-12')\n",
    "\n",
    "    for repo in repos:\n",
    "        # Title\n",
    "        title = repo.find('h3').text.strip()\n",
    "\n",
    "        # Description\n",
    "        description = repo.find('p', class_='col-9').text.strip()\n",
    "\n",
    "        # Contributors count\n",
    "        contributors_count = repo.find('a', href=lambda href: href and \"/contributors\" in href).text.strip()\n",
    "\n",
    "        # Language used\n",
    "        language = repo.find('span', itemprop='programmingLanguage')\n",
    "        language_used = language.text.strip() if language else \"Not specified\"\n",
    "\n",
    "        # Print details\n",
    "        print(\"Title:\", title)\n",
    "        print(\"Description:\", description)\n",
    "        print(\"Contributors Count:\", contributors_count)\n",
    "        print(\"Language Used:\", language_used)\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find trending repositories container.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b749246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find top 100 songs container.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the container with top 100 songs\n",
    "top_100_container = soup.find('div', class_='chart-list container-fluid')\n",
    "\n",
    "if top_100_container:\n",
    "    # Find all individual songs in the container\n",
    "    songs = top_100_container.find_all('div', class_='chart-list-item')\n",
    "\n",
    "    for song in songs:\n",
    "        # Song name\n",
    "        song_name = song.find('span', class_='chart-list-item__title-text').text.strip()\n",
    "\n",
    "        # Artist name\n",
    "        artist_name = song.find('div', class_='chart-list-item__artist').text.strip()\n",
    "\n",
    "        # Last week rank\n",
    "        last_week_rank = song.find('div', class_='chart-list-item__last-week').text.strip()\n",
    "\n",
    "        # Peak rank\n",
    "        peak_rank = song.find('div', class_='chart-list-item__weeks-at-one').text.strip()\n",
    "\n",
    "        # Weeks on board\n",
    "        weeks_on_board = song.find('div', class_='chart-list-item__weeks-on-chart').text.strip()\n",
    "\n",
    "        # Print details\n",
    "        print(\"Song Name:\", song_name)\n",
    "        print(\"Artist Name:\", artist_name)\n",
    "        print(\"Last Week Rank:\", last_week_rank)\n",
    "        print(\"Peak Rank:\", peak_rank)\n",
    "        print(\"Weeks on Board:\", weeks_on_board)\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find top 100 songs container.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda28f2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Book name\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m book_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Author name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m author_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the details of highest selling novels\n",
    "table = soup.find('table')\n",
    "\n",
    "if table:\n",
    "    # Find all rows in the table except the header row\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    for row in rows:\n",
    "        # Extracting data from each column of the row\n",
    "        columns = row.find_all('td')\n",
    "\n",
    "        # Book name\n",
    "        book_name = columns[1].text.strip()\n",
    "\n",
    "        # Author name\n",
    "        author_name = columns[2].text.strip()\n",
    "\n",
    "        # Volumes sold\n",
    "        volumes_sold = columns[3].text.strip()\n",
    "\n",
    "        # Publisher\n",
    "        publisher = columns[4].text.strip()\n",
    "\n",
    "        # Genre\n",
    "        genre = columns[5].text.strip()\n",
    "\n",
    "        # Print details\n",
    "        print(\"Book Name:\", book_name)\n",
    "        print(\"Author Name:\", author_name)\n",
    "        print(\"Volumes Sold:\", volumes_sold)\n",
    "        print(\"Publisher:\", publisher)\n",
    "        print(\"Genre:\", genre)\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find the table containing the details of highest selling novels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14aa0054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find TV series container.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the container with TV series details\n",
    "tv_series_container = soup.find('div', class_='lister-list')\n",
    "\n",
    "if tv_series_container:\n",
    "    # Find all individual TV series in the container\n",
    "    tv_series = tv_series_container.find_all('div', class_='lister-item-content')\n",
    "\n",
    "    for series in tv_series:\n",
    "        # Name\n",
    "        name = series.find('a').text.strip()\n",
    "\n",
    "        # Year span\n",
    "        year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "\n",
    "        # Genre\n",
    "        genre = series.find('span', class_='genre').text.strip()\n",
    "\n",
    "        # Run time\n",
    "        runtime = series.find('span', class_='runtime').text.strip()\n",
    "\n",
    "        # Ratings\n",
    "        ratings = series.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "\n",
    "        # Votes\n",
    "        votes = series.find('span', {'name': 'nv'}).text.strip()\n",
    "\n",
    "        # Print details\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Year Span:\", year_span)\n",
    "        print(\"Genre:\", genre)\n",
    "        print(\"Run Time:\", runtime)\n",
    "        print(\"Ratings:\", ratings)\n",
    "        print(\"Votes:\", votes)\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find TV series container.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc48fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the link to datasets page.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/lw6f6b7d16q2_yf4k_8hbpch0000gn/T/ipykernel_19486/4187970399.py:9: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  datasets_link = soup.find('a', text='View ALL Data Sets')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to datasets page\n",
    "datasets_link = soup.find('a', text='View ALL Data Sets')\n",
    "if datasets_link:\n",
    "    datasets_url = url + datasets_link['href']\n",
    "    datasets_response = requests.get(datasets_url)\n",
    "    datasets_soup = BeautifulSoup(datasets_response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the details of datasets\n",
    "    table = datasets_soup.find('table', class_='dataTable')\n",
    "\n",
    "    if table:\n",
    "        # Find all rows in the table except the header row\n",
    "        rows = table.find_all('tr')[1:]\n",
    "\n",
    "        for row in rows:\n",
    "            # Extracting data from each column of the row\n",
    "            columns = row.find_all('td')\n",
    "\n",
    "            # Dataset name\n",
    "            dataset_name = columns[0].text.strip()\n",
    "\n",
    "            # Data type\n",
    "            data_type = columns[1].text.strip()\n",
    "\n",
    "            # Task\n",
    "            task = columns[2].text.strip()\n",
    "\n",
    "            # Attribute type\n",
    "            attribute_type = columns[3].text.strip()\n",
    "\n",
    "            # Number of instances\n",
    "            num_instances = columns[4].text.strip()\n",
    "\n",
    "            # Number of attributes\n",
    "            num_attributes = columns[5].text.strip()\n",
    "\n",
    "            # Year\n",
    "            year = columns[6].text.strip()\n",
    "\n",
    "            # Print details\n",
    "            print(\"Dataset Name:\", dataset_name)\n",
    "            print(\"Data Type:\", data_type)\n",
    "            print(\"Task:\", task)\n",
    "            print(\"Attribute Type:\", attribute_type)\n",
    "            print(\"Number of Instances:\", num_instances)\n",
    "            print(\"Number of Attributes:\", num_attributes)\n",
    "            print(\"Year:\", year)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not find the table containing the details of datasets.\")\n",
    "else:\n",
    "    print(\"Could not find the link to datasets page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0a709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
