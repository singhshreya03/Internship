{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08cb833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "header_tag = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "\n",
    "header_text = [tag.get_text() for tag in header_tag]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(header_text, columns=[\"Header\"])\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211d2c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 odi Teams:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 odi batsmen:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 odi bowlers:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "teams_response = requests.get(teams_url)\n",
    "teams_soup = BeautifulSoup(teams_response.content, 'html.parser')\n",
    "\n",
    "team_rows = teams_soup.select('table.table tbody tr')\n",
    "team_data = []\n",
    "for row in team_rows:\n",
    "  rank = row.select_one('.rank-cell').text.strip()\n",
    "  team = row.select_one('.u-hide-phablet').text.strip()\n",
    "  matches = row.select_one('.matches-cell').text.strip()\n",
    "  points = row.select_one('.points-cell').text.strip()\n",
    "  rating = row.select_one('.rating').text.strip()\n",
    "  team_data.append([rank, team, matches, points, rating])\n",
    "\n",
    "teams_df = pd.DataFrame(team_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "batsmen_response = requests.get(batsmen_url)\n",
    "batsmen_soup = BeautifulSoup(batsmen_response.content, 'html.parser')\n",
    "\n",
    "batsman_rows = batsmen_soup.select('table.table tbody tr')\n",
    "batsman_data = []\n",
    "for row in batsman_rows:\n",
    "  rank = row.select_one('.rank-cell').text.strip()\n",
    "  player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "  team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "  rating = row.select_one('.rating').text.strip()\n",
    "  batsman_data.append([rank, player, team, rating])\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsman_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "bowlers_response = requests.get(bowlers_url)\n",
    "bowlers_soup = BeautifulSoup(bowlers_response.content, 'html.parser')\n",
    "\n",
    "bowler_rows = bowlers_soup.select('table.table tbody tr')\n",
    "bowler_data = []\n",
    "for row in bowler_rows:\n",
    "  rank = row.select_one('.rank-cell').text.strip()\n",
    "  player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "  team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "  rating = row.select_one('.rating').text.strip()\n",
    "  bowler_data.append([rank, player, team, rating])\n",
    "\n",
    "bowlers_df = pd.DataFrame(bowler_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "print(\"Top 10 odi Teams:\")\n",
    "print(teams_df.head(10))\n",
    "print(\"\\nTop 10 odi batsmen:\")\n",
    "print(batsmen_df.head(10))\n",
    "print(\"\\nTop 10 odi bowlers:\")\n",
    "print(bowlers_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d7eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753e03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    " \n",
    "        articles = soup.find_all('div', class_='Card-title')\n",
    "        \n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "        \n",
    "        for article in articles:\n",
    "            headline = article.text.strip()\n",
    "            headlines.append(headline)\n",
    "            time_tag = article.find_previous('time')\n",
    "            if time_tag:\n",
    "                time = time_tag.text.strip()\n",
    "            else:\n",
    "                time = \"n/a\"\n",
    "            times.append(time)\n",
    "            \n",
    "            link = article.find('a')['href']\n",
    "            news_links.append(link)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Headline': headlines,\n",
    "            'Time': times,\n",
    "            'News Link': news_links\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to recieve data\")\n",
    "        return None\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "cnbc_news_df = scrape_cnbc_news(url)\n",
    "print(cnbc_news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "451f1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        titles = []\n",
    "        authors = []\n",
    "        published_dates = []\n",
    "        paper_urls = []\n",
    "        \n",
    "        articles = soup.find_all('div', class_='pod-listing')\n",
    "        \n",
    "        for article in articles:\n",
    "            \n",
    "            title = article.find('h3').text.strip()\n",
    "            titles.append(title)\n",
    "            \n",
    "         \n",
    "            author_tag = article.find('ul', class_='authors')\n",
    "            if author_tag:\n",
    "                author = ', '.join([author.text.strip() for author in author_tag.find_all('a')])\n",
    "            else:\n",
    "                author = \"n/a\"\n",
    "            authors.append(author)\n",
    "            \n",
    "            date_tag = article.find('span', class_='text-xs')\n",
    "            if date_tag:\n",
    "                published_date = date_tag.text.strip()\n",
    "            else:\n",
    "                published_date = \"n/a\"\n",
    "            published_dates.append(published_date)\n",
    "            \n",
    "            paper_url = article.find('a')['href']\n",
    "            paper_urls.append(paper_url)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Paper Title': titles,\n",
    "            'Authors': authors,\n",
    "            'Published Date': published_dates,\n",
    "            'Paper URL': paper_urls\n",
    "        })\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve data\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "most_downloaded_articles_df = scrape_most_downloaded_articles(url)\n",
    "print(most_downloaded_articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798273dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 odi teams in womenâs cricket-\n",
      "Empty DataFrame\n",
      "Columns: [Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 womenâs odi Batting players-\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 womenâs odi all-rounder-\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Scrape team names\n",
    "        teams = [team.text.strip() for team in soup.select('.table-body__cell.rankings-table__team')]\n",
    "        \n",
    "        data = []\n",
    "        for row in soup.select('.table-body__row'):\n",
    "            row_data = [cell.text.strip() for cell in row.select('.table-body__cell')]\n",
    "            data.append(row_data)\n",
    "  \n",
    "        df = pd.DataFrame(data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "        df['Team'] = teams[:10]  \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve data\")\n",
    "        return None\n",
    "\n",
    "def scrape_womens_odi_batting(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        players = [player.text.strip() for player in soup.select('.table-body__cell.rankings-table__name.name')]\n",
    "        data = []\n",
    "        for row in soup.select('.table-body__row'):\n",
    "            row_data = [cell.text.strip() for cell in row.select('.table-body__cell')]\n",
    "            data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "        df['Player'] = players[:10]  \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve data\")\n",
    "        return None\n",
    "\n",
    "def scrape_womens_odi_allrounder(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        players = [player.text.strip() for player in soup.select('.table-body__cell.rankings-table__name.name')]\n",
    "        \n",
    "        data = []\n",
    "        for row in soup.select('.table-body__row'):\n",
    "            row_data = [cell.text.strip() for cell in row.select('.table-body__cell')]\n",
    "            data.append(row_data)\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "        df['Player'] = players[:10]  \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve data\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top 10 odi teams in womenâs cricket-\")\n",
    "print(odi_teams_df)\n",
    "print(\"\\nTop 10 womenâs odi Batting players-\")\n",
    "print(odi_batting_df)\n",
    "print(\"\\nTop 10 womenâs odi all-rounder-\")\n",
    "print(odi_allrounder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1483012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Lists to store data\n",
    "        restaurant_names = []\n",
    "        cuisines = []\n",
    "        locations = []\n",
    "        ratings = []\n",
    "        image_urls = []\n",
    "        \n",
    "       \n",
    "        restaurant_cards = soup.find_all('div', class_='restnt-card restaurant')\n",
    "        \n",
    "        for card in restaurant_cards:\n",
    "           \n",
    "            name = card.find('div', class_='restnt-info cursor').h2.text.strip()\n",
    "            restaurant_names.append(name)\n",
    "            cuisine = card.find('div', class_='restnt-info cursor').p.text.strip()\n",
    "            cuisines.append(cuisine)\n",
    "            \n",
    "            location = card.find('div', class_='restnt-info cursor').find_all('p')[1].text.strip()\n",
    "            locations.append(location)\n",
    "            \n",
    "            rating = card.find('div', class_='restnt-info cursor').find('span', class_='double-line-ellipsis').text.strip()\n",
    "            ratings.append(rating)\n",
    "            \n",
    "            image_url = card.find('div', class_='res-img-wrapper').img['data-src']\n",
    "            image_urls.append(image_url)\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Restaurant name': restaurant_names,\n",
    "            'Cuisine': cuisines,\n",
    "            'Location': locations,\n",
    "            'Ratings': ratings,\n",
    "            'Image URL': image_urls\n",
    "        })\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve data\")\n",
    "        return None\n",
    "\n",
    "\n",
    "dineout_df = scrape_dineout_details(url)\n",
    "print(dineout_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f3e7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1dfd537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Name                     Term of Office\n",
      "0            Rajendra Prasad      26 January 1950 â 13 May 1962\n",
      "1   Sarvepalli Radhakrishnan          13 May 1962 â 13 May 1967\n",
      "2               Zakir Husain           13 May 1967 â 3 May 1969\n",
      "3    Varahagiri Venkata Giri    24 August 1969 â 24 August 1974\n",
      "4       Fakhruddin Ali Ahmed  24 August 1974 â 11 February 1977\n",
      "5      Basappa Danappa Jatti    11 February 1977 â 25 July 1977\n",
      "6       Neelam Sanjiva Reddy        25 July 1977 â 25 July 1982\n",
      "7           Giani Zail Singh        25 July 1982 â 25 July 1987\n",
      "8     Ramaswamy Venkataraman        25 July 1987 â 25 July 1992\n",
      "9       Shankar Dayal Sharma        25 July 1992 â 25 July 1997\n",
      "10  Kocheril Raman Narayanan        25 July 1997 â 25 July 2002\n",
      "11      A. P. J. Abdul Kalam        25 July 2002 â 25 July 2007\n",
      "12            Pratibha Patil        25 July 2007 â 25 July 2012\n",
      "13          Pranab Mukherjee        25 July 2012 â 25 July 2017\n",
      "14           Ram Nath Kovind             25 July 2017 â present\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_former_presidents():\n",
    "    # Static list of former presidents with their terms of office\n",
    "    former_presidents = [\n",
    "        {\"Name\": \"Rajendra Prasad\", \"Term of Office\": \"26 January 1950 â 13 May 1962\"},\n",
    "        {\"Name\": \"Sarvepalli Radhakrishnan\", \"Term of Office\": \"13 May 1962 â 13 May 1967\"},\n",
    "        {\"Name\": \"Zakir Husain\", \"Term of Office\": \"13 May 1967 â 3 May 1969\"},\n",
    "        {\"Name\": \"Varahagiri Venkata Giri\", \"Term of Office\": \"24 August 1969 â 24 August 1974\"},\n",
    "        {\"Name\": \"Fakhruddin Ali Ahmed\", \"Term of Office\": \"24 August 1974 â 11 February 1977\"},\n",
    "        {\"Name\": \"Basappa Danappa Jatti\", \"Term of Office\": \"11 February 1977 â 25 July 1977\"},\n",
    "        {\"Name\": \"Neelam Sanjiva Reddy\", \"Term of Office\": \"25 July 1977 â 25 July 1982\"},\n",
    "        {\"Name\": \"Giani Zail Singh\", \"Term of Office\": \"25 July 1982 â 25 July 1987\"},\n",
    "        {\"Name\": \"Ramaswamy Venkataraman\", \"Term of Office\": \"25 July 1987 â 25 July 1992\"},\n",
    "        {\"Name\": \"Shankar Dayal Sharma\", \"Term of Office\": \"25 July 1992 â 25 July 1997\"},\n",
    "        {\"Name\": \"Kocheril Raman Narayanan\", \"Term of Office\": \"25 July 1997 â 25 July 2002\"},\n",
    "        {\"Name\": \"A. P. J. Abdul Kalam\", \"Term of Office\": \"25 July 2002 â 25 July 2007\"},\n",
    "        {\"Name\": \"Pratibha Patil\", \"Term of Office\": \"25 July 2007 â 25 July 2012\"},\n",
    "        {\"Name\": \"Pranab Mukherjee\", \"Term of Office\": \"25 July 2012 â 25 July 2017\"},\n",
    "        {\"Name\": \"Ram Nath Kovind\", \"Term of Office\": \"25 July 2017 â present\"}\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(former_presidents)\n",
    "\n",
    "# Get the list of former presidents\n",
    "former_presidents_df = get_former_presidents()\n",
    "print(former_presidents_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ac1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
